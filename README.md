# ğŸ§  Mixture of Experts (MoE) â€“ Classification Multi-Classe avec LLMs

> Architecture basÃ©e sur des experts spÃ©cialisÃ©s, pour une classification fine et contrÃ´lÃ©e Ã  lâ€™aide de modÃ¨les de langage.

---

## ğŸš§ Ã‰tat du projet

ğŸ› ï¸ **En cours de construction**  
Ce projet est en dÃ©veloppement actif. Les premiÃ¨res briques fonctionnelles sont en place :
- âœ… Fine-tuning d'experts One-vs-All avec diffÃ©rents LLMs
- âœ… SystÃ¨me d'infÃ©rence MoE avec gating soft
- ğŸ”œ IntÃ©gration du seuil EER et systÃ¨me de rejet
- ğŸ”œ Comparaison avec un classifieur global

---

## ğŸ“Œ Objectif

Construire un systÃ¨me de classification multi-classe en NLP en s'appuyant sur une architecture **Mixture of Experts** :
- Un modÃ¨le expert **par classe**
- Chacun entraÃ®nÃ© en **classification binaire**
- Prise de dÃ©cision via un **mÃ©canisme de gating** inspirÃ© des approches rÃ©centes (gating soft)

---

## ğŸ§± Structure du MoE

- **3 classes cibles** extraites du dataset :  
  `Food and Entertaining`, `Home and Garden`, `Personal Care and Style`
- **3 experts spÃ©cialisÃ©s** :
  - `bert-base-uncased`
  - `distilbert-base-uncased`
  - `roberta-base`
- **EntraÃ®nement individuel** via HuggingFace `Trainer`
- **PrÃ©diction finale** basÃ©e sur pondÃ©ration dynamique des scores (longueur du texte)

---

## ğŸ—‚ï¸ Contenu du projet

